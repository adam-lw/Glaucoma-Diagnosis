{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsKW4JgRG19o7oANLo11eZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adam-lw/Glaucoma-Diagnosis/blob/main/ViT_Base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Verify that we are using GPU rather than CPU for faster execution\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"Using cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"Using CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3WKSsY5WJUH",
        "outputId": "d9b3659e-a61d-4920-98f0-63d7e450b7a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiiMAqHbXemN",
        "outputId": "408d3f43-19cb-49a4-d296-e5567abfa930"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_3g4UhZX0W9",
        "outputId": "11224d6f-c9ba-488a-e378-046b30e042dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import math\n",
        "from torch.utils.data import random_split\n",
        "from tqdm import tqdm\n",
        "from transformers import ViTModel, ViTConfig, ViTForImageClassification, AutoModel, ViTImageProcessor\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "p-W-YnhetJKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VitExperiments\n",
        "Class for executing various Vision Transformer tests"
      ],
      "metadata": {
        "id": "-kBprojiRqtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTExperiments():\n",
        "\n",
        "    # Initialise datasets upon class creation\n",
        "    def __init__(self, train_data, test_data, val_data):\n",
        "      self.train_data = train_data\n",
        "      self.test_data = test_data\n",
        "      self.val_data = val_data\n",
        "\n"
      ],
      "metadata": {
        "id": "K5pwpfViSGLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Running Parameters\n",
        "### Model Configuration Parameters:\n",
        "\n",
        "*vitPretrainedModel:* PyTorch nn.Module object containing the architecture and weights of the pre-trained ViT\n",
        "\n",
        "*vitNumHeads:* Number of Self-Attention Heads. Should be equal to the pre-trained configuration unless we are testing head removal.\n",
        "\n",
        "*classifierHead:* Classification head to use on top of the ViT feature extractor.\n",
        "\n",
        "*weightFreezeProfile:* Configuration for various levels of weight freezing to reduce overfitting and increase inference speed.\n",
        "\n",
        "*embedding_dim:* Number of dimensions in the patch and position embedding, and thus the output size of the ViT before the MLP head.\n",
        "\n",
        "*num_classes:* Number of classes in the given dataset. Used for the length of the MLP classification head to provide a classification.\n",
        "\n",
        "\n",
        "### Data Augmentation & Normalisation Parameters:\n",
        "\n",
        "*train_Layer1Transform:* PyTorch nn.Transform responsible for image processing and data augmentation on overall image.\n",
        "\n",
        "*train_Layer2Transform:* PyTorch nn.Transform for processing and augmentation on 1st iteration \"windowed\" images from attention mechanism.\n",
        "\n",
        "*train_Layer3Transform:* PyTorch nn.Transform for processing and augmentation on 2nd iteration \"windowed\" images from attention mechanism.\n",
        "\n",
        "*test_Transform:* PyTorch nn.Transform for image processing on validation and test datasets.\n",
        "\n",
        "### Training Config:\n",
        "\n",
        "*lossFunction:* Which loss function to use.\n",
        "\n",
        "*optimiser:* Which optimiser algorithm to use\n",
        "\n",
        "*earlyStopThreshold:* The number of consecutive epochs where the validation set performance does not increase before we stop training\n",
        "\n",
        "\n",
        "### Training Hyperparameters:\n",
        "\n",
        "*batchSize:* How many images are processed before model weights are updated via backpropagation?\n",
        "\n",
        "*learningRate:* How quickly/slowly are weights updated during backpropagation?\n",
        "\n",
        "*dropoutRate:* Rate at which weights are randomly set to 0 during training to avoid overfitting\n",
        "\n",
        "### Model Saving/Loading:\n",
        "\n",
        "*savePath:* Path to file in which the model and optimiser's state_dicts are saved\n",
        "\n",
        "*loadPath:* (optional) Path to file in which model is loaded. This is for if we'd like to load a pre-existing model and continue training"
      ],
      "metadata": {
        "id": "vDTka0wNSAae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "GhjPQmrzs0sk",
        "outputId": "6c3b8f68-85fd-4096-b36b-6810ee9d1811"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d1138a9a119b>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    def runExperiment(self, train_data, test_data, val_data, train_Layer1Transform, train_Layer2Transform, train_Layer3Transform, testTransform, )\u001b[0m\n\u001b[0m                                                                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "    def runExperiment(self, vitPretrainedModel, vitNumHeads, classifierHead, weightFreezeProfile, embedding_dim, num_classes, \\\n",
        "                      train_Layer1Transform, train_Layer2Transform, train_Layer3Transform, test_transform, \\\n",
        "                      lossFunction, optimiser, earlyStopThreshold, \\\n",
        "                      batchSize, learningRate, dropoutRate):\n",
        "\n",
        "        # Initialise TensorBoard tracking\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "\n",
        "        # Create dataloaders from dataset and provided batchSize hyperparameter\n",
        "        train_dataloader = DataLoader(self.train_data, batch_size=batchSize, shuffle=True, pin_memory=True)\n",
        "        val_dataloader = DataLoader(self.val_data, batch_size=128, shuffle=False, pin_memory=True)\n",
        "        test_dataloader = DataLoader(self.test_data, batch_size=128, shuffle=False, pin_memory=True)\n",
        "\n",
        "        # TODO: attention head removal\n",
        "\n",
        "        # TODO: Weight freezing\n",
        "\n",
        "        # TODO: dropout rates\n",
        "\n",
        "        # Create model\n",
        "        vitModel = nn.Sequential(\n",
        "            vitPretrainedModel,\n",
        "            BaseModelOutputWithPoolingToTensor(),\n",
        "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "            classifierHead,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Load model onto GPU\n",
        "        vitModel.to(device)\n",
        "\n",
        "        # Initialise our optimiser function based on provided experiment parameters\n",
        "        modelOptimiser = optimiser(vitModel.parameters(), lr=learningRate)\n",
        "\n",
        "\n",
        "        bestLoss = None # Best loss score on validation dataset \n",
        "        lastImprovement = 0 # Epochs since last improvement\n",
        "\n",
        "        epochNum = 0 # Used for TensorBoard tracking\n",
        "\n",
        "        # Training loop\n",
        "        while lastImprovement < earlyStopThreshold:\n",
        "            \n",
        "            # Set model to training mode\n",
        "            vitModel.train()\n",
        "\n",
        "            # Loop through entire dataset, splitting into batches specified upon creation of train_dataloader\n",
        "            for batchTensor, labelTensor in tqdm(train_dataloader):\n",
        "                \n",
        "                # Ensure data is loaded on GPU\n",
        "                batchTensor = batchTensor.to(device)\n",
        "                labelTensor = labelTensor.to(device)\n",
        "                \n",
        "                # Reset previously calculated gradients\n",
        "                modelOptimiser.zero_grad()\n",
        "\n",
        "                # Calculate predicted labels based on training images\n",
        "                trainOutput = vitModel(batchTensor)\n",
        "\n",
        "                # Calculate loss based on difference between predicted and actual labels\n",
        "                loss = lossFunction(trainOutput, labelTensor)\n",
        "\n",
        "                # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "                loss.backward()\n",
        "\n",
        "                # Perform a single optimization step (parameter update)\n",
        "                modelOptimiser.step()\n",
        "            \n",
        "            # Update TensorBoard\n",
        "            writer.add_scalar(\"Training Loss\", loss.item(), global_step=epochNum)\n",
        "\n",
        "            \n",
        "\n",
        "            # Evaluate model performance on Validation set\n",
        "            vitModel.eval()\n",
        "\n",
        "            combinedValidationLoss = 0.0\n",
        "\n",
        "            # Disable gradient calculations when evaluating\n",
        "            with torch.no_grad():\n",
        "\n",
        "                # For each 128 image batch, calculate the loss, and add this to the combined validation loss\n",
        "                # The number of batches is arbitrary, it has no impact on model performance, but speeds up processing time\n",
        "                for batchTensor, labelTensor in tqdm(val_dataloader):\n",
        "\n",
        "                    # Calculate predicted labels on validation set\n",
        "                    validationOutputs = vitModel(batchTensor.to(device))\n",
        "\n",
        "                    # Calculate validation loss for the batch\n",
        "                    validationLoss = lossFunction(validationOutputs, labelTensor)\n",
        "\n",
        "                    # Add this to the combined total, weighted by the number of images in batch\n",
        "                    combinedValidationLoss += validationLoss.item() * batchTensor.size(0)\n",
        "            \n",
        "            combinedValidationLoss = combinedValidationLoss / len(self.val_data)\n",
        "\n",
        "            writer.add_scalar(\"Validation Loss\", combinedValidationLoss, global_step=epochNum)\n",
        "\n",
        "            epochNum += 1      \n",
        "\n",
        "\n",
        "            if bestLoss is None:\n",
        "                bestLoss = combinedValidationLoss\n",
        "                lastImprovement = 0\n",
        "\n",
        "                torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict()\n",
        "                    }, PATH)\n",
        "            elif combinedValidationLoss < bestLoss:\n",
        "                bestLoss = combinedValidationLoss\n",
        "                lastImprovement = 0\n",
        "            else:\n",
        "                lastImprovement += 1\n",
        "\n",
        "        \n",
        "\n",
        "        # Extract attention weights here\n",
        "        # Average them\n",
        "        # Calculate windows\n",
        "        # Optimise on windows, on copy of ViT\n",
        "\n",
        "        return \n",
        "\n",
        "\n",
        "    def evaluateModel(self, vitModel, dataset):\n",
        "        # Evaluation mode\n",
        "        vitModel.eval()\n",
        "\n",
        "        predictions = []\n",
        "        actuals = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batchTensor, labelTensor in tqdm(test_dataloader):\n",
        "                outputs = vitModel(batchTensor.to(device))\n",
        "                predictions.extend(torch.argmax(outputs, axis=1).tolist())\n",
        "                actuals.extend(labelTensor.tolist())\n",
        "                # Calculate windows here, etc\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Preds \" + str(predictions))\n",
        "        print(\"Actuals \" + str(actuals))\n",
        "\n",
        "        accuracy = accuracy_score(actuals, predictions)\n",
        "\n",
        "        precision = precision_score(actuals, predictions)\n",
        "\n",
        "        recall = recall_score(actuals, predictions)\n",
        "\n",
        "        f1 = f1_score(actuals, predictions)\n",
        "\n",
        "        print(\"Results:\")\n",
        "        print(\"Accuracy: \" + str(accuracy))\n",
        "        print(\"Precision: \" + str(precision))\n",
        "        print(\"Recall: \" + str(recall))\n",
        "        print(\"F1: \" + str(f1))\n",
        "\n",
        "    def calculateAUROC(self, resultsTensor)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModelOutputWithPoolingToTensor(nn.Module):\n",
        "    # Convert Hugging Face model outputs to tensor, for use in an nn.Sequential.\n",
        "\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "      return input.pooler_output"
      ],
      "metadata": {
        "id": "oNSf_t1oYD3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubsetWithTransform(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.subset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.subset)"
      ],
      "metadata": {
        "id": "SSH8PdXfX_cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customDataLoaders():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def mendeleyDataLoader(self):\n",
        "        pass\n",
        "\n",
        "    def goalsDataLoader(self, train_transform, test_transform, train_size, val_size, test_size):\n",
        "        labelledSet = datasets.ImageFolder(\"/content/drive/MyDrive/GOALS-Dataset/train/Train/Image\")\n",
        "        \n",
        "        train_data, val_data, test_data = random_split(labelledSet, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "        train_data = SubsetWithTransform(train_data, train_transform)\n",
        "        val_data = SubsetWithTransform(val_data, test_transform)\n",
        "        test_data = SubsetWithTransform(test_data, test_transform)\n",
        "\n",
        "        return train_data, val_data, test_data\n",
        "\n",
        "    def dukeDataLoader(self):\n",
        "        pass\n",
        "\n",
        "    def combinedOCTDataLoader(self):\n",
        "        "
      ],
      "metadata": {
        "id": "tGRoJL8tMeCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise training and testing transforms for image normalisation, resizing and data augmentation\n",
        "\n",
        "train_Layer1_transform = transforms.Compose([\n",
        "          transforms.Resize((224,224)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "      ])\n",
        "\n",
        "train_Layer2_transform = transforms.Compose([\n",
        "          transforms.Resize((224,224)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "      ])\n",
        "\n",
        "train_Layer3_transform = transforms.Compose([\n",
        "          transforms.Resize((224,224)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "      ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "          transforms.Resize((224,224)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "      ])\n",
        "\n",
        "# Load dataset with transforms\n",
        "dl = customDataLoaders()\n",
        "\n",
        "# 60% train, 20% validation, 20% test\n",
        "train, val, test = dl.goalsDataLoader(train_Layer1_transform, test_transform, 0.6, 0.2, 0.2)\n",
        "\n",
        "# Initialise our experiment manager with the train, validation and test data.\n",
        "experimentManager = ViTExperiments(train, val, test)\n",
        "\n",
        "# Specify embedding dimensionality\n",
        "embedding_dim = 768\n",
        "\n",
        "# Run an experiment with specified parameters\n",
        "experimentManager.runExperiment(\n",
        "    vitPretrainedModel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k'),\n",
        "    vitNumHeads = 12, \n",
        "    classifierHead = nn.Linear(in_features=embedding_dim, out_features=len(train.classes)),\n",
        "    weightFreezeProfile = None,\n",
        "    embedding_dim = embedding_dim,\n",
        "    num_classes = len(train.classes),\n",
        "    train_Layer1Transform = train_Layer1_transform,\n",
        "    train_Layer2Transform = train_Layer2_transform,\n",
        "    train_Layer3Transform = train_Layer3_transform,\n",
        "    test_transform = test_transform,\n",
        "    lossFunction = nn.CrossEntropyLoss,\n",
        "    optimiser = torch.optim.AdamW,\n",
        "    earlyStopThreshold = 5,\n",
        "    batchSize = 1024,\n",
        "    learningRate = 0.0001,\n",
        "    dropoutRate = None\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Remember to start saving torch checkpoints outside of colab!"
      ],
      "metadata": {
        "id": "TQlV-nmdMc6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}